import streamlit as st

from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_ollama.chat_models import ChatOllama
from langchain_core.prompts import PromptTemplate


# =====================================================
# CONFIGURA√á√ÉO STREAMLIT
# =====================================================
st.set_page_config(
    page_title="Or√°culo ECD",
    page_icon="üìò",
    layout="wide"
)

st.title("üìò Guru SPED Cont√°bil")
st.caption("Baseado exclusivamente no Manual Oficial da ECD")


# =====================================================
# EMBEDDINGS + VECTOR STORE (DEVE SER IGUAL AO INDEXADOR)
# =====================================================
@st.cache_resource
def load_vectorstore():
    embeddings = OllamaEmbeddings(
        model="nomic-embed-text"  # üî¥ TEM que ser o mesmo do indexador
    )

    vectorstore = Chroma(
        persist_directory="db",
        embedding_function=embeddings
    )

    return vectorstore


vectorstore = load_vectorstore()
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})


# =====================================================
# MODELO LLM (LEVE PARA CODESPACES)
# =====================================================
llm = ChatOllama(
    model="llama3.2:3b",
    temperature=0
)


# =====================================================
# PROMPT DO OR√ÅCULO
# =====================================================
PROMPT_TEMPLATE = """
Voc√™ √© um especialista s√™nior em Escritura√ß√£o Cont√°bil Digital (ECD),
com profundo conhecimento do Manual da ECD e das regras do SPED Cont√°bil.

REGRAS OBRIGAT√ìRIAS:
- Responda SOMENTE com base no contexto fornecido.
- Cite explicitamente Bloco, Registro e se√ß√£o quando aplic√°vel.
- Se a informa√ß√£o n√£o existir no contexto, diga claramente:
  "Essa informa√ß√£o n√£o consta no Manual da ECD fornecido."
- Seja t√©cnico, objetivo e preciso.
- N√£o invente regras.
- Quando poss√≠vel, traduza a regra para l√≥gica computacional.

CONTEXTO:
{context}

PERGUNTA:
{question}

RESPOSTA:
"""

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=PROMPT_TEMPLATE
)


# =====================================================
# INTERFACE
# =====================================================
question = st.text_input(
    "Digite sua d√∫vida sobre a ECD:",
    placeholder="Ex: Para que serve o Bloco 0?"
)

if question:
    with st.spinner("üîé Consultando o Manual da ECD..."):

        # ---------------------------
        # BUSCA SEM√ÇNTICA
        # ---------------------------
        docs = retriever.invoke(question)

        if not docs:
            st.warning("Nenhum trecho relevante encontrado no manual.")
        else:
            context = "\n\n".join(doc.page_content for doc in docs)

            final_prompt = prompt.format(
                context=context,
                question=question
            )

            # ---------------------------
            # GERA√á√ÉO DA RESPOSTA
            # ---------------------------
            try:
                response = llm.invoke(final_prompt)

                st.subheader("üìñ Resposta do Or√°culo")
                st.write(response.content)

            except Exception as e:
                st.error("‚ùå Erro ao gerar resposta com o modelo.")
                st.exception(e)
